{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando Clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd_clients = spark.read.format(\n",
    "   \"com.databricks.spark.csv\").option(\n",
    "   \"header\", \"true\").option(\n",
    "   \"inferSchema\", \"true\").option(\n",
    "   \"delimiter\", ',').load(\n",
    "   's3://treinamento-big-data/clients.csv')\n",
    "rdd_clients.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Verificando Metadados da tabela\n",
    "rdd_clients.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Verificando 20 linhas da tabela\n",
    "rdd_clients.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd_payments = spark.read.format(\n",
    "   \"com.databricks.spark.csv\").option(\n",
    "   \"header\", \"true\").option(\n",
    "   \"inferSchema\", \"true\").option(\n",
    "   \"delimiter\", ',').load(\n",
    "   's3://treinamento-big-data/payments.csv')\n",
    "   \n",
    "rdd_payments.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd_payments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd_payments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd_loan = spark.read.format(\n",
    "   \"com.databricks.spark.csv\").option(\n",
    "   \"header\", \"true\").option(\n",
    "   \"inferSchema\", \"true\").option(\n",
    "   \"delimiter\", ',').load(\n",
    "   's3://treinamento-big-data/loans.csv')\n",
    "rdd_loan.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd_loan.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "rdd_loan.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "payments_dfm = rdd_payments.registerTempTable(\"payments_dfm\")\n",
    "clients_dfm = rdd_clients.registerTempTable(\"clients_dfm\")\n",
    "loans_dfm = rdd_loan.registerTempTable(\"loans_dfm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "             \"\"\"SELECT \n",
    "                    *,\n",
    "                    cast(loan_start as date) as DT_INICIO,\n",
    "                    cast(loan_end as date) as DT_FIM,\n",
    "                    datediff(cast(loan_end as date),cast(loan_start as date)) as QT_DIAS_START_END,\n",
    "                    round(datediff(cast(loan_end as date),cast(loan_start as date))/30.5) as QT_MESES_START_END,\n",
    "                    round(datediff(cast(loan_end as date),cast(loan_start as date))/365) as QT_ANOS_START_END\n",
    "                FROM\n",
    "                    loans_dfm \n",
    "                WHERE \n",
    "                    client_id = 46109 and loan_type in ('home') \n",
    "                order by cast(loan_start as date)    \n",
    "             \"\"\"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "             \"\"\"SELECT \n",
    "                    *,\n",
    "                    cast(payment_date as date ) as DT_PGTO\n",
    "                    --min(payment_date) as DT_PRIMEIRO_PAGAMENTO,\n",
    "                    --max(payment_date) as DT_ULTIMO_PAGAMENTO,\n",
    "                FROM\n",
    "                    payments_dfm \n",
    "                WHERE \n",
    "                    loan_id = 10243 \n",
    "             \"\"\"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "             \"\"\"SELECT\n",
    "                    loan_id,\n",
    "                    sum(case when missed = 1 then 0 else payment_amount end) as VL_PGTO_MISS,\n",
    "                    sum(case when missed = 0 then 0 else payment_amount end) as VL_PGTO_NOT_MISS,\n",
    "                    sum(payment_amount) as VL_TOT,\n",
    "                    cast(min(payment_date) as date) as DT_PRIMEIRO_PAGAMENTO,\n",
    "                    cast(max(payment_date) as date) as DT_ULTIMO_PAGAMENTO,\n",
    "                    count(*) as QT_PGTOS\n",
    "                FROM\n",
    "                    payments_dfm \n",
    "                WHERE \n",
    "                    loan_id = 10243\n",
    "                GROUP BY  \n",
    "                    loan_id\n",
    "             \"\"\"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "clients_00 = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            *,\n",
    "                            round(log(income),4) as VL_LOG_RENDA,\n",
    "                            cast(joined as date) as DT_JOINED,\n",
    "                            substr(cast(joined as date),6,2) as VL_MES_JOINED\n",
    "                        FROM\n",
    "                            clients_dfm                     \n",
    "                     \"\"\"\n",
    "                     )\n",
    "clients_00_dfm = clients_00.registerTempTable(\"clients_00_dfm\")                     \n",
    "clients_00.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "loans_00 = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            client_id,\n",
    "                            round(avg(loan_amount),4) as VL_MED_EMP_CLI,\n",
    "                            min(loan_amount) as VL_MIN_EMP_CLI,\n",
    "                            max(loan_amount) as VL_MAX_EMP_CLI\n",
    "                        FROM\n",
    "                            loans_dfm  \n",
    "                        group by client_id     \n",
    "                     \"\"\"\n",
    "                     )\n",
    "loans_00_dfm = loans_00.registerTempTable(\"loans_00_dfm\")                         \n",
    "loans_00.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "clientes_01 = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            a.*,\n",
    "                            b.VL_MED_EMP_CLI,\n",
    "                            b.VL_MIN_EMP_CLI,\n",
    "                            b.VL_MAX_EMP_CLI\n",
    "                        FROM\n",
    "                            clients_00_dfm as a  \n",
    "                        left join  loans_00_dfm as b\n",
    "                        on a.client_id = b.client_id\n",
    "                     \"\"\"\n",
    "                     )\n",
    "                     \n",
    "clientes_01.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "loans_02 = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            a.*,\n",
    "                            b.payment_amount,\n",
    "                            b.payment_date,\n",
    "                            b.missed\n",
    "                        FROM\n",
    "                            loans_dfm as a  \n",
    "                        left join  payments_dfm as b\n",
    "                        on a.loan_id = b.loan_id\n",
    "                     \"\"\"\n",
    "                     )\n",
    "loans_02_dfm = loans_02.registerTempTable(\"loans_02_dfm\")                       \n",
    "loans_02.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "loans_03 = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            loan_type,\n",
    "                            round(avg(payment_amount),2) as VL_MED_PAGAMENTOS,\n",
    "                            count(*) as QT_CONTRATOS_TIPO\n",
    "                        FROM\n",
    "                            loans_02_dfm \n",
    "                        group by 1  \n",
    "                     \"\"\"\n",
    "                     )\n",
    "loans_03_dfm = loans_03.registerTempTable(\"loans_03_dfm\")                       \n",
    "loans_03.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "loans_04 = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            client_id,\n",
    "                            sum(case when loan_type in ('cash') then 1 else 0 end) as QT_EMP_CASH,\n",
    "                            avg(case when loan_type in ('cash') then loan_amount else 0 end) as VL_MEDEMP_CASH,\n",
    "                            count(*) as QT_CONTRATOS_TIPO\n",
    "                        FROM\n",
    "                            loans_02_dfm \n",
    "                        group by 1  \n",
    "                     \"\"\"\n",
    "                     )\n",
    "loans_04_dfm = loans_04.registerTempTable(\"loans_04_dfm\")                       \n",
    "loans_04.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import date, timedelta\n",
    "dataref = datetime.now()-timedelta(0, 0, 0, 0, 0, +2)\n",
    "anomesdia = dataref.strftime(\"%Y%m%d\")\n",
    "dataref\n",
    "#print('Data Hoje: ',dataref.strftime(\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "etl_final = spark.sql(\n",
    "                     \"\"\"SELECT \n",
    "                            *,\n",
    "                            '{}' as PK_DATREF\n",
    "                        FROM\n",
    "                            loans_04_dfm \n",
    "                     \"\"\".format(anomesdia)\n",
    "                     )\n",
    "             \n",
    "etl_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "etl_final.write.parquet(\"etl_final_00.parquet\")\n",
    "# Verificar os arquivos no sistema hdfs:\n",
    "# hdfs dfs -ls /user/zeppelin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "hive_context = HiveContext(sc)\n",
    "\n",
    "# Salvando a tabela como temporária no Hive\n",
    "#hive_context.registerDataFrameAsTable(etl_final, \"etl_final_00\")\n",
    "\n",
    "# Salvando a tabela no schema (database) treinamento como permanente do Hive (modo append)\n",
    "etl_final.write.mode('append').saveAsTable(\"default.etl_final_00\")\n",
    "hive_context.sql(\"show tables in default\").show()\n",
    "#hive_context.sql(\"drop table etl_final_01_\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"clients_00_dfm\") \n",
    "spark.catalog.dropTempView(\"clients_dfm\") \n",
    "spark.catalog.dropTempView(\"loans_00_dfm\") \n",
    "spark.catalog.dropTempView(\"loans_02_dfm\") \n",
    "spark.catalog.dropTempView(\"loans_03_dfm\") \n",
    "spark.catalog.dropTempView(\"loans_04_dfm\") \n",
    "spark.catalog.dropTempView(\"loans_dfm\") \n",
    "spark.catalog.dropTempView(\"payments_dfm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "hive_context.sql(\"show tables in default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "tab_hive = hive_context.sql(\"select * from default.etl_final_00\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Salvando a tabela no schema (database) treinamento como permanente do Hive (modo append) particionado\n",
    "etl_final.write.mode('append').partitionBy('PK_DATREF').saveAsTable(\"default.etl_final_01\")\n",
    "\n",
    "hive_context.sql(\"show tables in default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# Ler partição Hive\n",
    "df_hive_parquet = sqlContext.read.option(\"mergeSchema\", \"true\").parquet(\"/user/hive/warehouse/etl_final_01/PK_DATREF=20200117\")\n",
    "df_hive_parquet.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "df_hive_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "nm_path_s3 = 's3://treinamento-big-data/'\n",
    "Tabela='UserEJL'+anomesdia\n",
    "etl_final.write.partitionBy('PK_DATREF').parquet(nm_path_s3 + Tabela , mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print('Exercício')\n",
    "#Metadados:\n",
    "#PK_CLIENTE\n",
    "#Valor medio, minimo e maximo de contratos por tipo de emprestimo\n",
    "#Valor médio pago por tipo de empréstimo\n",
    "#Valor médio não pago por tipo empréstimo\n",
    "#Duração (meses) média dos contratos \n",
    "#Quantidade média de pagamentos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
